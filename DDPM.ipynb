{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import glob\n",
    "from huggingface_hub import login\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from diffusers import UNet2DModel\n",
    "from diffusers import DDPMScheduler\n",
    "from torch.nn.parallel import DataParallel\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from diffusers import DDPMPipeline\n",
    "import math\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import torch.multiprocessing as mp\n",
    "from accelerate import notebook_launcher\n",
    "import torch.nn.functional as F\n",
    "from torch.multiprocessing import Pool, Process, set_start_method\n",
    "\n",
    "login(\"\")\n",
    "\n",
    "try:\n",
    "    mp.set_start_method(\"spawn\")\n",
    "except RuntimeError:\n",
    "    print('no')\n",
    "    pass  \n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 512  # 생성되는 이미지 해상도\n",
    "    train_batch_size = 1\n",
    "    eval_batch_size = 16  # 평가 동안에 샘플링할 이미지 수\n",
    "    num_epochs = 1000\n",
    "    gradient_accumulation_steps = 8\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 100\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = \"fp16\"  # `no`는 float32, 자동 혼합 정밀도를 위한 `fp16`\n",
    "    output_dir = \"test_ddpm\"  # 로컬 및 HF Hub에 저장되는 모델명\n",
    "\n",
    "    push_to_hub = False  # 저장된 모델을 HF Hub에 업로드할지 여부\n",
    "    hub_private_repo = False\n",
    "    overwrite_output_dir = True  # 노트북을 다시 실행할 때 이전 모델에 덮어씌울지\n",
    "    seed = 36\n",
    "\n",
    "config = TrainingConfig()\n",
    "    \n",
    "image_path = np.sort(glob.glob(os.path.join('./','*.png')))\n",
    "\n",
    "transform = A.Compose([  \n",
    "    A.RandomGamma(gamma_limit=(90, 110), p=0.1),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.1),\n",
    "    A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array(Image.open(self.image_paths[idx]))\n",
    "        file_name = os.path.basename(self.image_paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            #(c, w ,h) -> (w, h, c)\n",
    "            image = augmented['image']\n",
    "            \n",
    "        # image.clone().detach().to(torch.float32).requires_grad_(True)\n",
    "        \n",
    "        return image\n",
    "\n",
    "train_dataset = CustomDataset(image_path, transform)\n",
    "# DataLoader 설정 (배치 크기 32)\n",
    "dataloader = DataLoader(train_dataset, batch_size=config.train_batch_size, shuffle=False)\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # 타겟 이미지 해상도\n",
    "    in_channels=3,  # 입력 채널 수, RGB 이미지에서 3\n",
    "    out_channels=3,  # 출력 채널 수\n",
    "    layers_per_block=3,  # UNet 블럭당 몇 개의 ResNet 레이어가 사용되는지\n",
    "    block_out_channels=(64, 64, 128, 128, 512, 512),  # 각 UNet 블럭을 위한 출력 채널 수\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # 일반적인 ResNet 다운샘플링 블럭\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # spatial self-attention이 포함된 일반적인 ResNet 다운샘플링 블럭\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # 일반적인 ResNet 업샘플링 블럭\n",
    "        \"AttnUpBlock2D\",  # spatial self-attention이 포함된 일반적인 ResNet 업샘플링 블럭\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(dataloader) * config.num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataloader:\n",
    "    _ = 1\n",
    "    break\n",
    "\n",
    "sample_image = i[0].squeeze(0)\n",
    "noise = torch.randn(sample_image.shape)\n",
    "timesteps = torch.LongTensor([50])\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
    "\n",
    "# 2개의 subplot 생성\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# 원본 이미지와 노이즈가 추가된 이미지를 각 subplot에 그리기\n",
    "axes[1].imshow(((noisy_image.permute(0, 1, 2) + 1.0) * 127.5).type(torch.uint8).numpy()[0], cmap='gray')\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis('off')  # 축 숨기기\n",
    "\n",
    "axes[0].imshow(((sample_image.permute(0, 1, 2) + 1.0) * 127.5).type(torch.uint8).numpy()[0], cmap='gray')\n",
    "axes[1].set_title(\"Noisy Image\")\n",
    "axes[1].axis('off')  # 축 숨기기\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(images, rows, cols, size=(128, 128)): \n",
    "    \n",
    "    grid_size = (rows, cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_size[0], grid_size[1], figsize=(12, 12))\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = images[i].permute(1, 2, 0)  # (3, 512, 512) -> (512, 512, 3)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')  # 축 제거\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    image_list=[]\n",
    "    for i in range(config.eval_batch_size):\n",
    "        image = pipeline(\n",
    "            batch_size=1,  \n",
    "            generator=torch.manual_seed(config.seed + i), \n",
    "        ).images[0]\n",
    "        \n",
    "        image_tensor = ToTensor()(image)\n",
    "        image_list.append(image_tensor)\n",
    "        \n",
    "    image_grid = make_grid(image_list, rows=4, cols=4, size=(128, 128)) \n",
    "\n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.savefig(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        if config.push_to_hub:\n",
    "            repo_id = create_repo(\n",
    "                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True\n",
    "            ).repo_id\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images =  batch\n",
    "\n",
    "            noise = torch.randn(clean_images.shape, device=clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "        \n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "        \n",
    "            with accelerator.accumulate(model):\n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                loss = loss / config.gradient_accumulation_steps\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                #gradient_accumulation_steps\n",
    "                if (step + 1) % config.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    lr_scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                evaluate(config, epoch, pipeline)\n",
    "\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                if config.push_to_hub:\n",
    "                    upload_folder(\n",
    "                        repo_id=repo_id,\n",
    "                        folder_path=config.output_dir,\n",
    "                        commit_message=f\"Epoch {epoch}\",\n",
    "                        ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
    "                    )\n",
    "                else:\n",
    "                    pipeline.save_pretrained(config.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = (config, model, noise_scheduler, optimizer, dataloader, lr_scheduler)\n",
    "notebook_launcher(train_loop, args, num_processes=1)\n",
    "\n",
    "# 결과 이미지 시각화\n",
    "pipeline = DDPMPipeline.from_pretrained(\"./test_ddpm\")\n",
    "pipeline.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_image = pipeline(num_inference_steps=1000).images[0]\n",
    "    \n",
    "plt.imshow(generated_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
